{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DoHyung08/RL/blob/main/0406myEnv/designing_my_environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x0bbwqJ-JlW"
      },
      "source": [
        "# 강화 학습 환경 복습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVLBBrcY-JlX",
        "outputId": "f22128ca-7050-44bf-c1ac-c83f53b66abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[classic-control]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[classic-control])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.5.2)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[classic-control]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSGsqtpI-JlY"
      },
      "source": [
        "### 강화 학습 문제를 직접 풀어낼 정책 정의\n",
        "\n",
        "강화 학습에서는 어떤 함수를 학습하고자 하는 걸까요? 에이전트 안에는 상태 관측값(입력)을 받고 그것을 앞으로 취해야 할 최적의 행동(출력)에 매핑하는 함수가 있습니다. 예를 들어, 미로 속 에이전트의 현재 상태가 $(2, 3)$ 좌표라면, 에이전트 안의 함수는 이 입력값을 \"오른쪽으로 이동\"이라는 출력값에 매핑하는 것이 될 수 있습니다. 이 함수를 $\\pi$라고 한다면, 아래와 같이 수식으로 쓸 수 있습니다.\n",
        "$$\n",
        "\\pi((2, 3)) = \\text{\"오른쪽으로 이동\"}\n",
        "$$\n",
        "강화 학습 용어로 이 함수를 정책(policy)이라고 부릅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt-xKZSn-JlZ"
      },
      "outputs": [],
      "source": [
        "def policy(state):\n",
        "    x_pos, y_pos = state\n",
        "    if x_pos == 2 and y_pos == 3:\n",
        "        return +1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaT3XWWj-JlZ"
      },
      "source": [
        "### 강화 학습이 돌아가는 환경의 코드 복습\n",
        "\n",
        "1. 인공지능 모델은 환경의 현재 상태(state)를 관찰할 수 있습니다. 미로 찾기 문제에서 환경의 현재 상태란 미로 속 현재 위치를 의미합니다. 예를 들어, 모델이 미로의 $(2, 3)$ 좌표에 있다면, 이 좌표는 현재 상태를 나타냅니다.\n",
        "\n",
        "2. 인공지능 모델은 관찰된 상태로부터 앞으로 취할 행동(action)을 결정합니다. 양갈래 길 중에서 어디로 갈지 결정하는 것 등이 그 예시가 될 수 있습니다.\n",
        "\n",
        "3. 환경은 상태를 변경(transition)시키고 그 행동에 대한 보상(reward)을 생성합니다. 인공지능 모델은 그 상태와 보상을 다 받습니다. 미로 찾기 문제에서 환경의 변화란 인공지능 모델의 (앞선 결정에 따른) 미로 속 위치 변화를 의미합니다. 예를 들어, '오른쪽으로 이동' 행동을 취하면, 에이전트의 위치 좌표가 $(2, 3)$에서 $(2, 4)$로 바뀔 수 있습니다. 보상은 출구를 찾았을 때 주어지는 경품이나 막다른 길에 도달했을 때 받는 페널티 등을 생각해 볼 수 있습니다.\n",
        "\n",
        "4.  이 새로운 정보(환경의 변화와 이에 따른 보상)를 사용하여 인공지능은 그런 행동이 좋아 그걸 반복해야 하는지, 또는 좋지 않아 회피해야 하는지 결정할 수 있습니다. 완료될 때까지 (done) 이 관측-행동-보상 사이클은 계속됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLpOoh_z-JlZ",
        "outputId": "593e8921-4c8b-46c7-fb8a-559785917ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: [-0.52625793  0.        ]\n",
            "Chose action: 0\n",
            "New state: [-0.527238   -0.00098006]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5291908  -0.00195276]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5321016  -0.00291083]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.53594863 -0.00384706]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5407031  -0.00475446]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5463293  -0.00562623]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5527852  -0.00645588]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5600225  -0.00723727]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5679871  -0.00796463]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5766198 -0.0086327]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.58585656 -0.00923672]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.59562904 -0.0097725 ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6058655  -0.01023646]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6164912  -0.01062571]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6274292 -0.010938 ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.638601  -0.0111718]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6499273  -0.01132629]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6613286  -0.01140135]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.67272615 -0.01139752]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.68404216 -0.011316  ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.69520074 -0.0111586 ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7061285  -0.01092769]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.71675456 -0.01062611]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.72701174 -0.01025717]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.73683625 -0.00982451]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7461683 -0.0093321]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7549525  -0.00878413]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7631374  -0.00818497]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.77067655 -0.00753911]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.77752763 -0.00685111]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7836532  -0.00612557]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7890203  -0.00536708]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7936005  -0.00458021]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.79737    -0.00376949]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.80030936 -0.00293938]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.80240375 -0.00209433]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.80364245 -0.00123872]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-8.0401933e-01 -3.7689201e-04]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-8.0353254e-01  4.8681960e-04]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.8021844  0.0013481]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.79998183  0.00220261]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7969358  0.003046 ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7930619   0.00387388]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7883801   0.00468182]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.78291476  0.00546533]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7766949   0.00621988]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.769754    0.00694089]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7621302   0.00762378]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.75386626  0.00826394]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7450094   0.00885683]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7356115   0.00939797]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.72572845  0.009883  ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7154207   0.01030776]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7047524   0.01066833]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.69379133  0.01096108]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.68260854  0.01118278]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6712779   0.01133064]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6598755   0.01140235]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.64847934  0.01139619]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6371683   0.01131104]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.62602186  0.01114643]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6151193   0.01090257]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6045389   0.01058038]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.59435743  0.01018149]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5846492  0.0097082]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5754857   0.00916352]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5669346  0.0085511]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5590594   0.00787521]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.55191875  0.00714066]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5455659   0.00635281]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5400485   0.00551744]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5354077   0.00464076]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.53167844  0.00372931]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5288885  0.0027899]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.52705896  0.00182957]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5262034   0.00085553]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-5.2632833e-01 -1.2494002e-04]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5274328  -0.00110447]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.52950853 -0.00207571]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5325399  -0.00303139]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.53650427 -0.00396434]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5413718  -0.00486758]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5471062  -0.00573434]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5536644  -0.00655818]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.56099737 -0.00733299]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.56905043 -0.00805309]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.57776374 -0.00871326]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.58707255 -0.00930881]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.59690815 -0.00983563]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.60719836 -0.01029023]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6178682  -0.01066978]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6288403  -0.01097215]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6400362  -0.01119589]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6513765  -0.01134026]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6627817  -0.01140522]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6741731  -0.01139142]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.68547326 -0.01130013]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.69660646 -0.01113323]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7074996  -0.01089315]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7180824  -0.01058281]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.728288   -0.01020554]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.738053   -0.00976505]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7473183  -0.00926532]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.75602895 -0.00871059]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.76413417 -0.00810523]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7715879  -0.00745375]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7783486  -0.00676072]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.78437936 -0.00603073]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.78964776 -0.00526838]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.794126   -0.00447822]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.79779077 -0.00366478]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.80062324 -0.00283252]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.80260915 -0.00198589]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.8037384  -0.00112924]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-8.040053e-01 -2.669339e-04]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-8.034086e-01  5.967077e-04]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.8019512   0.00145736]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.79964054  0.0023107 ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.79648817  0.00315236]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7925102   0.00397795]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7877272   0.00478303]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7821641   0.00556311]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7758505   0.00631364]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7688204   0.00703006]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.76111263  0.00770776]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7527705   0.00834215]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.74384177  0.0089287 ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7343789   0.00946293]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7244383   0.00994052]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.714081    0.01035732]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7033716   0.01070944]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6923782   0.01099332]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6811725   0.01120576]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6698285   0.01134404]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6584225   0.01140592]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6470328   0.01138976]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.63573825  0.0112945 ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.62461853  0.01111977]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6137526   0.01086587]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6032188   0.01053381]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5930935  0.0101253]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5834508   0.00964274]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.57436156  0.00908923]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.56589305  0.00846848]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5581082   0.00778483]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.551065    0.00704319]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5448161   0.00624896]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5394081   0.00540798]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5348816  0.0045265]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5312705   0.00361111]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5286018   0.00266864]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5268957   0.00170616]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.52616477  0.00073089]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-5.2641463e-01 -2.4986485e-04]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5276434  -0.00122875]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.52984184 -0.00219841]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.53299344 -0.00315159]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.53707457 -0.00408114]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.54205465 -0.0049801 ]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5478964  -0.00584175]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5545561  -0.00665968]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.56198394 -0.00742783]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.5701245  -0.00814058]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.57891726 -0.00879277]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.58829707 -0.00937978]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.59819466 -0.00989759]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.60853744 -0.01034278]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.61925006 -0.01071261]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.63025504 -0.01100503]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.64147377 -0.01121869]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.65282667 -0.01135293]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6642345  -0.01140782]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6756186  -0.01138407]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6869016  -0.01128304]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.6980083  -0.01110667]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.70886576 -0.01085747]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.71940416 -0.01053841]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.72955704 -0.01015288]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7392617  -0.00970462]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.74845934 -0.00919766]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7570956  -0.00863623]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7651203  -0.00802475]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.77248806 -0.00736773]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.77915776 -0.00666973]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7850931  -0.00593537]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.79026234 -0.00516922]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7946382  -0.00437585]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.798198   -0.00355976]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.8009234  -0.00272543]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.80280066 -0.00187728]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.8038204  -0.00101967]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-8.0397731e-01 -1.5695415e-04]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-8.0327076e-01  7.0654735e-04]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.8017042   0.00156651]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.79928565  0.00241861]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7960272   0.00325847]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.79194546  0.00408169]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.78706163  0.00488384]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7814012   0.00566041]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7749944   0.00640685]\n",
            "Reward: -1.0\n",
            "Chose action: 0\n",
            "New state: [-0.7678758   0.00711859]\n",
            "Reward: -1.0\n",
            "Final state: [-0.7678758   0.00711859]\n",
            "Total reward: -201.0\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "state, _ = env.reset()\n",
        "print(\"Initial state:\", state)\n",
        "\n",
        "done = False\n",
        "total_reward = 0\n",
        "while not done:\n",
        "    action = policy(state) # Step 1-2: Observes the state and chooses an action\n",
        "    print(\"Chose action:\", action)\n",
        "    state, reward, done, _, _ = env.step(action) # Step 3: Environment returns the next state and reward\n",
        "    total_reward += reward\n",
        "    print(\"New state:\", state)\n",
        "    print(\"Reward:\", reward)\n",
        "    if total_reward < -200:\n",
        "        break\n",
        "\n",
        "print(\"Final state:\", state)\n",
        "print(\"Total reward:\", total_reward)\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h17LyV10-Jla"
      },
      "source": [
        "# 환경 코드 예시 살펴보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6LrITwP-Jla"
      },
      "source": [
        "### 복도를 걸어다니며 배정된 방까지 이동하는 환경\n",
        "강화 학습 환경을 더 직관적으로 이해하기 위해 직접 환경 코드를 만들어 봅시다. 강화 학습의 환경을 만들기 위해서는 먼저 상태 공간 $S$와 행동 공간 $A$를 정의해야 합니다.\n",
        "\n",
        "예를 들어, 일자형 복도에서 배정받은 방을 찾아 돌아다니는 환경을 생각해봅시다. 여러분은 왼쪽 또는 오른쪽으로 이동할 수 있습니다. 이 환경에서 상태와 행동은 아래와 같이 표현할 수 있습니다.\n",
        "\\begin{equation}\n",
        "S = \\{(i, j): i, j \\in \\{\\text{Room 101}, \\cdots, \\text{Room 106}\\}\\}, \\quad A = \\{\\text{left}, \\text{right}\\}\n",
        "\\end{equation}\n",
        "즉, 환경의 상태는 현재 에이전트의 위치 뿐 아니라 배정받은 방이 어디인지도 표현할 수 있어야 합니다.\n",
        "이를 코드로 보면 아래와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCgAOHEJ-Jla"
      },
      "outputs": [],
      "source": [
        "rooms = list(range(101, 107)) # [101, ..., 106]\n",
        "state_space = [(i, j) for i in rooms for j in rooms]\n",
        "action_space = [-1, 1] # left, right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DSz3Qw3-Jla"
      },
      "source": [
        "맨 끝 방에서는 반대 방향으로만 이동할 수 있고, 복도 밖으로 이동하려고 해도 벽에 부딪혀 더 움직이지 못합니다. 이를 코드로 구현하면 아래와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nO-NEVY-Jlb"
      },
      "outputs": [],
      "source": [
        "def transition(state, action):\n",
        "    current_location, my_room = state\n",
        "    next_location = current_location + action # moves with prob. 1\n",
        "    next_location = max(next_location, 101) # can't move left\n",
        "    next_location = min(next_location, 106) # can't move right\n",
        "    next_state = (next_location, my_room)\n",
        "    return next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phCe9xVJ-Jlb"
      },
      "source": [
        "다음으로 보상을 정의합니다. 여러분이 배정받은 초록색 방에 도착하면 1의 보상을 받고 환경은 종료됩니다. 그 외의 경우에는 보상이 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvr4PUzU-Jlb"
      },
      "outputs": [],
      "source": [
        "def reward_function(state, action):\n",
        "    next_state = transition(state, action)\n",
        "    next_location, my_room = next_state\n",
        "    if next_location == my_room:\n",
        "        return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BE70JZD-Jlb"
      },
      "source": [
        "이제 이 환경을 코드로 구현하려면, 크게 두 가지 함수를 정의해야합니다. 첫째, 환경을 생성할 때 에이전트가 처음 관찰할 상태를 제공하는 함수를 만들어야 합니다. 둘째, 에이전트가 환경을 선택했을 때, 다음 상태와 보상을 제공하는 함수를 만들어야 합니다. 아래 코드는 이 두 가지를 각각 $\\texttt{reset}$과 $\\texttt{step}$ 함수에 구현한 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZKim_e9-Jlb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Defines our corridor environment\n",
        "class CorridorEnv:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.start = 101\n",
        "        self.end = 120\n",
        "        rooms = list(range(self.start, self.end+1))\n",
        "        self.state_space = [(i, j) for i in rooms for j in rooms]\n",
        "        self.action_space = [-1, 1]\n",
        "\n",
        "    # 위의 코드와 동일!\n",
        "    def transition(self, state, action):\n",
        "        current_location, my_room = state\n",
        "        next_location = current_location + action\n",
        "        next_location = max(next_location, self.start)\n",
        "        next_location = min(next_location, self.end)\n",
        "        next_state = (next_location, my_room)\n",
        "        return next_state\n",
        "\n",
        "    # 위의 코드와 동일!\n",
        "    def reward_function(self, state, action):\n",
        "        next_state = transition(state, action)\n",
        "        next_location, my_room = next_state\n",
        "        if next_location == my_room:\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    def reset(self):\n",
        "        same_start_and_end = True\n",
        "        while same_start_and_end:\n",
        "            state = random.choice(self.state_space)\n",
        "            same_start_and_end = (state[0] == state[1])\n",
        "        self.state = state\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.transition(self.state, action)\n",
        "        reward = self.reward_function(self.state, action)\n",
        "        done = (next_state[0] == next_state[1])\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC1w43v9-Jlc",
        "outputId": "b1e709a0-abdb-4d31-930f-223d538e32c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: (108, 120)\n",
            "292\n"
          ]
        }
      ],
      "source": [
        "# Sample code for running the environment\n",
        "env = CorridorEnv()\n",
        "state = env.reset()\n",
        "done = False\n",
        "print(\"Initial state:\", state)\n",
        "tr = 0\n",
        "while not done:\n",
        "    action = np.random.choice(env.action_space)\n",
        "    #print(\"Chose action:\", action)\n",
        "    state, reward, done = env.step(action)\n",
        "    #print(\"New state:\", state)\n",
        "    #print(\"Reward:\", reward)\n",
        "    tr += 1\n",
        "print(tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wHBcWam-Jlc"
      },
      "source": [
        "Q. 조금 상황을 바꿔서 복도를 걸어다니는 취한 손님에 대한 환경을 구현해봅시다. 이 환경은 $\\texttt{CorridorEnv}$와 거의 동일하지만, 상태 변화에 확률이 추가됩니다. 에이전트는 똑같이 왼쪽 혹은 오른쪽으로 움직일 수 있지만, 이 에이전트는 취해있기 때문에 $20\\%$의 확률로 선택한 방향과 반대 방향으로 움직입니다. 이 취한 손님에 대한 환경 $\\texttt{DrunkenCorridorEnv}$를 구현해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71Rj2cFB-Jlc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Defines our corridor environment\n",
        "class DrunkenCorridorEnv:\n",
        "    def __init__(self):\n",
        "        self.start = 101\n",
        "        self.end = 120\n",
        "        rooms = list(range(self.start, self.end+1))\n",
        "        self.state_space = [(i, j) for i in rooms for j in rooms]\n",
        "        self.action_space = [-1, 1]\n",
        "\n",
        "    def transition(self, state, action):\n",
        "        if random.randint(0,100)<=20:\n",
        "          action = action * -1\n",
        "\n",
        "        current_location, my_room = state\n",
        "        next_location = current_location + action\n",
        "        next_location = max(next_location, self.start)\n",
        "        next_location = min(next_location, self.end)\n",
        "        next_state = (next_location, my_room)\n",
        "        return next_state\n",
        "\n",
        "    def reward_function(self, state, action):\n",
        "        next_state = transition(state, action)\n",
        "        next_location, my_room = next_state\n",
        "        if next_location == my_room:\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        same_start_and_end = True\n",
        "        while same_start_and_end:\n",
        "            state = random.choice(self.state_space)\n",
        "            same_start_and_end = (state[0] == state[1])\n",
        "        self.state = state\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.transition(self.state, action)\n",
        "        reward = self.reward_function(self.state, action)\n",
        "        done = (next_state[0] == next_state[1])\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = DrunkenCorridorEnv()\n",
        "state = env.reset()\n",
        "done = False\n",
        "print(\"Initial state:\", state)\n",
        "tr = 0\n",
        "while not done:\n",
        "    action = np.random.choice(env.action_space)\n",
        "    #print(\"Chose action:\", action)\n",
        "    state, reward, done = env.step(action)\n",
        "    #print(\"New state:\", state)\n",
        "    #print(\"Reward:\", reward)\n",
        "    tr += 1\n",
        "\n",
        "print(tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR88ANbaUSom",
        "outputId": "78b42035-bcb6-49f9-975c-3b4997e4d38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: (118, 119)\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9y7N6vp-Jlc"
      },
      "source": [
        "### 수직선 위의 에이전트\n",
        "에이전트는 원점의 위치에서 출발하여 보상의 총합을 최대화 하도록 움직이고 싶어합니다. 매 순간 에이전트는 왼쪽 또는 오른쪽으로 이동할 수 있으며, 총 세 번 만 움직일 수 있습니다. 이 때마다 에이전트는 그 위치에 적혀있는 보상을 받습니다. 아래 코드를 통해 에이전트가 놓여있는 환경을 이해해 볼까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGa4BytR-Jlc"
      },
      "outputs": [],
      "source": [
        "class MyEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.observation_space = gym.spaces.Discrete(7, start=-3)\n",
        "        self.action_space = gym.spaces.Discrete(2)\n",
        "        self.num_steps = 0\n",
        "\n",
        "    def reset(self):\n",
        "        state = 0\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        self.num_steps += 1\n",
        "\n",
        "        if action == 0:\n",
        "            next_state = state - 1\n",
        "        else:\n",
        "            next_state = state + 1\n",
        "\n",
        "        if next_state > 3:\n",
        "            next_state = 3\n",
        "        elif next_state < -3:\n",
        "            next_state = -3\n",
        "\n",
        "        reward = {\n",
        "            -3: 1,\n",
        "            -2: 1,\n",
        "            -1: 1,\n",
        "            0: 0,\n",
        "            1: -1,\n",
        "            2: -1,\n",
        "            3: 10\n",
        "        }[next_state]\n",
        "\n",
        "        done = self.num_steps >= 3\n",
        "        return next_state, reward, done, {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGFCGioB-Jld"
      },
      "source": [
        "근시안적인 관점에서 보면, 에이전트가 왼쪽으로 이동해야 당장 더 큰 보상을 받을 수 있습니다. 반대로 오른쪽으로 이동하게 되면 당장은 손해인 것처럼 보입니다. 하지만 총 세 번을 움직일 수 있는 상황에서는 오히려 두 번의 손해를 보고 나서야 비로소 가장 큰 보상을 받을 수 있게 됩니다. 즉, 장기적인 관점에서 에이전트는 보상의 총합을 최대화하기 위해 당장의 손해를 감수해야만 합니다. 이처럼 에이전트는 단순히 현재 상황에서의 최고의 선택을 고르는 것이 아니라 다음 상태까지 모두 고려한 최선의 선택을 내려야합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMlmC9vg-Jld"
      },
      "source": [
        "### 무슨 환경일까요?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6D_OOqG-Jld",
        "outputId": "6e03b591-74af-40e8-8dc3-00fec214f8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: 0\n",
            "Choose action from (0, 1, 2, 3): 1\n",
            "Chose action: 1\n",
            "New state: 4\n",
            "Reward: 0.0\n",
            "Choose action from (0, 1, 2, 3): 1\n",
            "Chose action: 1\n",
            "New state: 8\n",
            "Reward: 0.0\n",
            "Choose action from (0, 1, 2, 3): 2\n",
            "Chose action: 2\n",
            "New state: 9\n",
            "Reward: 0.0\n",
            "Choose action from (0, 1, 2, 3): 1\n",
            "Chose action: 1\n",
            "New state: 13\n",
            "Reward: 0.0\n",
            "Choose action from (0, 1, 2, 3): 1\n",
            "Chose action: 1\n",
            "New state: 13\n",
            "Reward: 0.0\n",
            "Choose action from (0, 1, 2, 3): 2\n",
            "Chose action: 2\n",
            "New state: 14\n",
            "Reward: 0.0\n",
            "Choose action from (0, 1, 2, 3): 2\n",
            "Chose action: 2\n",
            "New state: 15\n",
            "Reward: 1.0\n",
            "Game over!\n",
            "Final state: 15\n",
            "Total reward: 1.0\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False)\n",
        "state, _ = env.reset()\n",
        "print(\"Initial state:\", state)\n",
        "\n",
        "done = False\n",
        "total_reward = 0\n",
        "while not done:\n",
        "    action = int(input(\"Choose action from (0, 1, 2, 3): \"))\n",
        "    print(\"Chose action:\", action)\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    print(\"New state:\", state)\n",
        "    print(\"Reward:\", reward)\n",
        "\n",
        "print(\"Game over!\")\n",
        "print(\"Final state:\", state)\n",
        "print(\"Total reward:\", total_reward)\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hus-sOoH-Jld",
        "outputId": "a2e80386-bfb2-4a9f-b8a8-3e8dd9495955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: 0\n",
            "Choose action from (0, 1, 2, 3): 2\n",
            "Chose action: 2\n",
            "New state: 1\n",
            "Reward: 0.0\n",
            "Choose action from (0, 1, 2, 3): 2\n",
            "Chose action: 2\n",
            "New state: 5\n",
            "Reward: 0.0\n",
            "Game over!\n",
            "Final state: 5\n",
            "Total reward: 0.0\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True) # True로 바뀌면 어떻게 되나요?\n",
        "state, _ = env.reset()\n",
        "print(\"Initial state:\", state)\n",
        "\n",
        "done = False\n",
        "total_reward = 0\n",
        "while not done:\n",
        "    action = int(input(\"Choose action from (0, 1, 2, 3): \"))\n",
        "    print(\"Chose action:\", action)\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    print(\"New state:\", state)\n",
        "    print(\"Reward:\", reward)\n",
        "\n",
        "print(\"Game over!\")\n",
        "print(\"Final state:\", state)\n",
        "print(\"Total reward:\", total_reward)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2ZD7eD1-Jld"
      },
      "source": [
        "# 나만의 강화 학습 환경 만들어보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2K8znHt-Jle"
      },
      "outputs": [],
      "source": [
        "class MyEnv:\n",
        "    def __init__(self):\n",
        "        self.state_space = ???\n",
        "        self.action_space = ???\n",
        "\n",
        "    def transition(self, state, action):\n",
        "        next_state = ???\n",
        "        self.state = next_state\n",
        "        return next_state\n",
        "\n",
        "    def reward_function(self, state, action):\n",
        "        ???\n",
        "        return reward\n",
        "\n",
        "    def reset(self):\n",
        "        ???\n",
        "        return state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.transition(self.state, action)\n",
        "        reward = self.reward_function(self.state, action)\n",
        "        done = ???\n",
        "        return next_state, reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#환경\n",
        "한컴타자 해상구조 sos\n",
        "\n",
        "매 단계마다\n",
        "상대는 일정 확률로 튜브를 얻음\n",
        "플레이어와 상대 모두 튜브 1개씩을 잃음\n",
        "\n",
        "\n",
        "##상태\n",
        "플레이어와 상대방의 튜브 개수\n",
        "각자 0이 되면 진다.\n",
        "동시에 0이되면 비긴다.\n",
        "\n",
        "\n",
        "##행동\n",
        "상대의 튜브를 없앤다\n",
        "-> 상대 튜브 없어질 확률 조정\n",
        "\n",
        "튜브를 먹어 자신의 튜브를 쌓는다.\n",
        "-> 튜브가 먹힐 확률 조정\n",
        "\n",
        "\n",
        "##보상\n",
        "지면 -1\n",
        "이기면 1\n",
        "진행중이거나 비기면 0\n",
        "\n"
      ],
      "metadata": {
        "id": "qjVFIsoR-upw"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rllib",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}